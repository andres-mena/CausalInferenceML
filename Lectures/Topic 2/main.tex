\documentclass[xcolor=svgnames,t]{beamer} 
\usepackage[utf8]{inputenc}
\usepackage{booktabs, comment}
\usepackage{graphicx}  % Add graphicx package
\usepackage[absolute, overlay]{textpos} 
\usepackage{pgfpages}
\usepackage[font=footnotesize]{caption}
\useoutertheme{infolines} 
\usepackage{xcolor}
% \usepackage{cite}  % REMOVE this line because it conflicts with natbib
\usepackage{colortbl}
\definecolor{brownbrown}{RGB}{8, 8, 9}
\usepackage[round, sort, authoryear]{natbib}  % Use only natbib for citation management
\definecolor{brownred}{RGB}{198, 198, 198}

\setbeamercolor{title in head/foot}{bg=brownred, fg=brownbrown}
\setbeamercolor{author in head/foot}{bg=myuniversity}
\setbeamertemplate{page number in head/foot}{}

\usepackage{amsmath}
\usepackage[makeroom]{cancel}

\newtheorem{equi}{} %Creates a grey box when equi is called
\setbeamertemplate{navigation symbols}{} 
\usepackage{textpos}

\usepackage{tikz}

\usetheme{Madrid}
\definecolor{myuniversity}{RGB}{48, 67, 180}
\usecolortheme[named=myuniversity]{structure}
\usepackage{tikz}


\usepackage{colortbl} 
\newcommand{\myitem}{\item[$\circ$]}
\newcommand{\witem}{\item[\textcolor{white}{$\bullet$}]}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\AtBeginSection[]{
\begin{frame}
\frametitle{Content}
\tableofcontents[currentsection]
\end{frame}
}

\title[Randomized Experiment]{Randomized Experiment}
\subtitle{}
%\titlegraphic{\includegraphics[height=1cm]{brown-logo.png}}  % This line is commented out to remove the logo
\author[CIML ]{Causal Inference using Machine Learning\\ Master in Economics, UNT}
\institute[]{Andres Mena}
\date{Spring 2024}

\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}

\begin{document}
\begin{frame}
\maketitle
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\logo{\includegraphics[height=0.5cm]{brown-arms.png}}  % This line is commented out to remove the logo
%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents
\end{frame}

\section{Origins of Randomized Experiments}
\begin{frame}{The first RCT}
    \begin{block}{}
        "Let us divide them in halves, let us cast lots, that one half of them may fall to my share, and the other to yours; I will cure them without bloodletting and sensible evacuation; but do you do as ye know [...] we shall see how many Funerals both of us shall have."\\
        – Jan Baptist van Helmont, 1624
    \end{block}
    \begin{itemize}[<+->]
        \item \textbf{Van Helmont (17th century)}: Suggested dividing patients by lot to compare treatments, an early hint of experimental control.
        \item \textbf{Peirce (1885)}: Used random sequencing in psychology to prevent bias from expectations, anticipating randomization principles.
        \item \textbf{Gossett and Fisher (1920s)}: Gossett mentioned random plot placement; Fisher formalized randomization as essential for causal inference.
    \end{itemize}
\end{frame}
% Frame for Neyman (1923)
\begin{frame}{ \cite{neyman1923}}
    \begin{itemize}[<+->]
        \item Introduced the concept of potential outcomes to define causal effects.
        \item Applied potential outcomes specifically in the context of randomized experiments.
        \item Developed notation for potential yields in agricultural experiments, allowing estimation across different treatment groups.
        \item Emphasized the role of assignment mechanisms in calculating causal effects.
        \item Proposed an estimator for the Variance of the Average Treatment Effect (ATE) in randomized experiments.
    \end{itemize}
\end{frame}

% Frame for Fisher (1935)
\begin{frame}{\cite{fisher1935}}
    \begin{itemize}[<+->]
        \item Established randomization as the fundamental basis for valid causal inference.
        \item Introduced the concept of statistical significance and p-values within the framework of randomized experiments.
        \item Developed randomization techniques like randomized blocks, which became standard in experimental design.
        \item Emphasized the need for physical randomization to eliminate confounding variables.
        \item Proposed methods for testing hypotheses in a controlled experimental setup.
    \end{itemize}
\end{frame}

% Contrast Frame
\begin{frame}{Comparison: Neyman vs. Fisher}
    \begin{itemize}[<+->]
        \item \textbf{Neyman (1923)} focused on potential outcomes to estimate causal effects, but Fisher (1935) emphasized randomization as the basis for inference.
        \item Neyman treated randomization as a theoretical basis for probabilistic analysis, while Fisher emphasized physical randomization as essential for credible causal inference, making it a core requirement of experimental validity.
        \item Fisher introduced significance testing and p-values for general hypothesis, while Neyman was more concerned with unbiased estimation of ATE.
      
        \item Together, they laid the groundwork for randomized experiments and causal inference.
    \end{itemize}
\end{frame}
\section{Classification of Assignment Mechanisms}

\begin{frame}{Definition of Assignment Vector \( D \)}
    \begin{itemize}[<+->]
        \item \textbf{Assignment Vector}: A vector representing the treatment assignment for each unit in a study.
        \item For \( N \) units, \( D \) is an \( N \)-vector where \( D_i = d \) if unit \( i \) receives the treatment $d$.
        \item For two treatment groups, \( D \) is a binary vector with \( 2^N \) possible values.
    \end{itemize}
    
   
\end{frame}


\begin{frame}{Assignment Mechanism}
    \textbf{Assignment Mechanism:} Given a population of \( N \) units, the assignment mechanism is a row-exchangeable function, denoted as \( \Pr(D | X, Y(0), Y(1)) \), which takes values in the interval \([0, 1]\) and satisfies:
    
    \[
    \sum_{D \in \{0,1\}^N} \Pr(D | X, Y(0), Y(1)) = 1
    \]

    for all possible values of \( X \) (covariates), \( Y(0) \), and \( Y(1) \) (potential outcomes). (Row-exchangeability implies that the order of units within vectors or matrices is irrelevant to the function \( \Pr(\cdot) \).)
   
\end{frame}

\begin{frame}{Example: Assignment Mechanism with Two Units}
    Define the \textbf{treatment effect} for unit \( i \) as:
    \(
    \tau_i = Y_i(1) - Y_i(0)
    \)

    \[
    \Pr(D | X, Y(0), Y(1)) = 
    \begin{cases} 
      1 & \text{if } \tau_2 > \tau_1 \text{ and } D = \begin{bmatrix} 0 \\ 1 \end{bmatrix} \\[10pt]
      1 & \text{if } \tau_2 < \tau_1 \text{ and } D = \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\[10pt]
      \frac{1}{2} & \text{if } \tau_2 = \tau_1 \text{ and } D \in \left\{ \begin{bmatrix} 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \end{bmatrix} \right\} \\[10pt]
      0 & \text{if } D \in \left\{ \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 1 \end{bmatrix} \right\} \\[10pt]
      0 & \text{if } \tau_2 < \tau_1 \text{ and } D = \begin{bmatrix} 0 \\ 1 \end{bmatrix} \\[10pt]
      0 & \text{if } \tau_2 > \tau_1 \text{ and } D = \begin{bmatrix} 1 \\ 0 \end{bmatrix}
    \end{cases}
    \]
\end{frame}


\begin{frame}{Unit Assignment Probability}
    The \textbf{unit-level assignment probability} for unit \( i \) is defined as:

    \[
    p_i(X, Y(0), Y(1)) = \sum_{D : D_i = 1} \Pr(D | X, Y(0), Y(1)),
    \]

  
\end{frame}



\begin{frame}{Propensity Score}
    The \textbf{propensity score} at \( x \) is the average unit assignment probability for units with \( X_i = x \). It is defined as:

    \[
    e(x) = \frac{1}{N(x)} \sum_{i : X_i = x} p_i(X, Y(0), Y(1)),
    \]


\end{frame}
\begin{frame}{Example: Propensity Score}
\scriptsize
   \[
    \begin{array}{ll@{\hspace{2cm}}ll}
    \text{1.} \quad D = (0, 0, 0, 0) & P(D = 1) = 0 & \text{9.} \quad D = (1, 0, 1, 0) & P(D = 9) = \frac{3}{16} \\

    \text{2.} \quad D = (1, 0, 0, 0) & P(D = 2) = \frac{3}{16} & \text{10.} \quad D = (1, 0, 0, 1) & P(D = 10) = \frac{2}{16} \\

    \text{3.} \quad D = (0, 1, 0, 0) & P(D = 3) = \frac{2}{16} & \text{11.} \quad D = (0, 1, 0, 1) & P(D = 11) = \frac{2}{16} \\

    \text{4.} \quad D = (0, 0, 1, 0) & P(D = 4) = 0 & \text{12.} \quad D = (1, 1, 1, 0) & P(D = 12) = 0 \\

    \text{5.} \quad D = (0, 0, 0, 1) & P(D = 5) = 0 & \text{13.} \quad D = (1, 0, 1, 1) & P(D = 13) = 0 \\

    \text{6.} \quad D = (1, 1, 0, 0) & P(D = 6) = \frac{1}{16} & \text{14.} \quad D = (0, 1, 1, 1) & P(D = 14) = 0 \\

    \text{7.} \quad D = (0, 1, 1, 0) & P(D = 7) = \frac{2}{16} & \text{15.} \quad D = (1, 1, 0, 1) & P(D = 15) = 0 \\

    \text{8.} \quad D = (0, 0, 1, 1) & P(D = 8) = \frac{1}{16} & \text{16.} \quad D = (1, 1, 1, 1) & P(D = 16) = 0 \\
    \end{array}
    \]
\end{frame}
\section{Restrictions to Assignments}
\begin{frame}{Individualistic Assignment}
    \textbf{Definition 3.4 (Individualistic Assignment):}For some function \( q( \cdot ) \in [0, 1] \):
    
    \[
    p_i(X, Y(0), Y(1)) = q(X_i, Y_i(0), Y_i(1)), \quad \text{for all } i = 1, \ldots, N,
    \]

    and

    \[
    \Pr(D | X, Y(0), Y(1)) = c \cdot \prod_{i=1}^N q(X_i, Y_i(0), Y_i(1))^{D_i} (1 - q(X_i, Y_i(0), Y_i(1)))^{1 - D_i},
    \]
    \pause
\begin{itemize}
    \item The constant \( c \) ensures that the probabilities sum to unity.
    \pause
    \textcolor{red}{\textbf{Homework:} Compute the value of \( c \) for a generic assignment mechanism with two units and a binary treatment.}

   \end{itemize}
    
    
   
\end{frame}

% Slide 2: Probabilistic Assignment Mechanism
\begin{frame}{Probabilistic Assignment Mechanism}
    \textbf{Probabilistic Assignment Mechanism:} Under this mechanism, each unit has a non-zero probability of being assigned to either treatment or control, ensuring randomness in the assignment process.
    
    \[
    0 < \Pr(D_i = 1 | X, Y(0), Y(1)) < 1 \quad \text{for all units } i
    \]
    
   
\end{frame}

% Slide 3: Unconfounded Assignment Mechanism
\begin{frame}{Unconfounded Assignment Mechanism}
    \textbf{Unconfounded Assignment Mechanism:} This mechanism assumes that assignment to treatment is independent of the potential outcomes, given the covariates. In other words, the assignment is “as good as random” conditional on covariates.
    
    \[
    \Pr(D | X, Y(0), Y(1)) = \Pr(D | X)
    \]
   
    \pause
    \begin{itemize}
       
    
        \item  Given individualistic assignment and unconfoundedness\[
\Pr(D | X, Y(0), Y(1)) = c \cdot \prod_{i=1}^N q(X_i)^{D_i} \cdot (1 - q(X_i))^{1 - D_i} 
\]

so that

\[
e(x) = q(x)
\]
    \end{itemize}
        
    
\end{frame}

\section{Types of Randomized Experiments}


\begin{frame}{Randomized Experiment}
    A classical randomized experiment is a randomized experiment with an assignment mechanism that is:
    
    \begin{itemize}
        \item (i) \textbf{Individualistic}: Each unit’s treatment assignment depends only on its own covariates and potential outcomes, independent of other units.
        \pause
        \item (ii) \textbf{Unconfounded}: Assignment to treatment is independent of potential outcomes given covariates, meaning assignment is “as good as random” conditional on covariates.
    \end{itemize}
\end{frame}

\begin{frame}{Bernoulli Trials}
    A \textbf{Bernoulli trial} is a classical randomized experiment where each unit is independently assigned to treatment or control, often based on a coin toss.
    
    \begin{itemize}
        \item Each unit has a probability \( q \) of being assigned to treatment and \( 1 - q \) of being assigned to control.
       \pause
        \item Each unit's assignment is independent of others, meaning the assignment for one unit does not affect the assignment for another.
        \pause 
        \item The assignment mechanism is:
            \begin{itemize}
                \item \textbf{Individualistic}: Each unit's assignment depends only on its own characteristics.
                \item \textbf{Probabilistic}: Each unit has a non-zero chance of receiving either treatment or control.
                \item \textbf{Unconfounded}: Given covariates, assignment does not depend on potential outcomes.
                \item \textbf{Controlled by the Researcher}: The probability \( q \) is specified by the researcher.
            \end{itemize}
    \end{itemize}
\end{frame}

% Bernoulli Trials: Slide 2 - Computation of Probability
\begin{frame}{Bernoulli Trials - Probability of an Assignment Vector}
    For a Bernoulli trial, the probability of an assignment vector \( D \) for \( N \) units is given by:
    
    \[
    \Pr(D | X, Y(0), Y(1)) = \prod_{i=1}^N \left( e(X_i)^{D_i} \cdot (1 - e(X_i))^{1 - D_i} \right)
    \]
    
    where:
    \begin{itemize}
        \item \( D_i = 1 \) if unit \( i \) is assigned to treatment, \( D_i = 0 \) otherwise.
        \item \( e(X_i) \): Propensity Score for unit \( i \).
    \end{itemize}
    If \( e(X_i)= q = 0.5 \), then \( \Pr(D | X, Y(0), Y(1)) = 0.5^N \).
\end{frame}


\begin{frame}{Completely Randomized Experiment - Definition}
    A \textbf{completely randomized experiment} assigns a fixed number \( N_t \) of units to treatment, and the remaining \( N - N_t \) units to control.
    
    \begin{itemize}
        \item The assignment is achieved by randomly selecting \( N_t \) units from a pool of \( N \) units.
        \item Ensures a balanced distribution of treated and control units, with exactly \( N_t \) in treatment and \( N - N_t \) in control.
        \item Each unit's assignment is \text{NOT} independent of others, but the total number of treated units is fixed by design.
        \item The assignment mechanism is:
            \begin{itemize}
                \item \textbf{Probabilistic}: Each unit has a positive probability of being selected for treatment or control.
                \item \textbf{Unconfounded}: Given covariates, assignment does not depend on potential outcomes.
                \item \textbf{Controlled by the Researcher}: The number \( N_t \) of treated units is specified by the researcher.
            \end{itemize}
    \end{itemize}
\end{frame}



% Completely Randomized Experiment: Slide 2 - Probability of an Assignment Vector
\begin{frame}{Completely Randomized Experiment - Probability of an Assignment Vector}
    In a completely randomized experiment, the probability of an assignment vector \( D \) is:
    
    \[
    \Pr(D | X, Y(0), Y(1)) = \begin{cases}
        \frac{1}{\binom{N}{N_t}} & \text{if } \sum_{i=1}^N D_i = N_t \\
        0 & \text{otherwise}
    \end{cases}
    \]
    
    where \( N_t \) is the predetermined number of units assigned to treatment.
\end{frame}

% Stratified Randomized Experiment: Slide 1 - Definition
\begin{frame}{Stratified Randomized Experiment - Definition}
    A \textbf{stratified randomized experiment} divides the population into blocks or strata based on covariates, and performs a completely randomized experiment within each block.
    
    \begin{itemize}
        \item Divides the population into strata so that units within each stratum are similar with respect to certain covariates.
        \pause
        \item Performs complete randomization within each stratum, ensuring balanced treatment and control within each block.
        \pause
        \item Reduces variability and improves the precision of causal inference estimates.
        \pause
        \item The goal is to reduce variance in the estimator and increase the power of statistical tests, enhancing the study's ability to detect treatment effects.
    \end{itemize}
\end{frame}

% Stratified Randomized Experiment: Slide 2 - Probability of an Assignment Vector
\begin{frame}{Stratified Randomized Experiment - Probability of an Assignment Vector}
    For a stratified randomized experiment with \( J \) blocks, the probability of an assignment vector \( D \) is:

    \[
    \Pr(D | X, Y(0), Y(1)) = \prod_{j=1}^J \frac{1}{\binom{N(j)}{N_t(j)}}
    \]

    where:
    \begin{itemize}
        \item \( N(j) \): Number of units in block \( j \),
        \item \( N_t(j) \): Number of treated units in block \( j \).
    \end{itemize}
\end{frame}

% Paired Randomized Experiment: Slide 1 - Definition
\begin{frame}{Paired Randomized Experiment - Definition}
    A \textbf{paired randomized experiment} is an extreme form of stratified randomization, where each block (or stratum) contains exactly two units.
    
    \begin{itemize}[<+->]
        \item Within each pair, one unit is randomly assigned to treatment and the other to control, ensuring direct comparison between similar units.
        \item Ensures close matching on covariates within each pair, which helps to control for confounding variables.
        \item Minimizes differences between treated and control units on covariates, reducing bias in the estimated treatment effect.
        \item Reduces variance in the estimator by closely aligning treatment and control units.
        \item Increases statistical power by enhancing the precision of the causal inference, making it easier to detect treatment effects.
    \end{itemize}
\end{frame}

% Paired Randomized Experiment: Slide 2 - Probability of an Assignment Vector
\begin{frame}{Paired Randomized Experiment - Probability of an Assignment Vector}
    For a paired randomized experiment with \( N/2 \) pairs, the probability of an assignment vector \( D \) is:

    \[
    \Pr(D | X, Y(0), Y(1)) = 2^{-\frac{N}{2}}
    \]

    - Each unit within a pair has an equal probability of being assigned to treatment or control.
\end{frame}

\begin{frame}{Number of Possible Values for the Assignment Vector by Design and Sample Size}
    \begin{table}[]
        \centering
        \scriptsize  % Reduce font size for the entire table
        \begin{tabular}{@{}lcccccc@{}}
            \toprule
            \shortstack{\textbf{Type of Experiment} \\ \textbf{and Design}} & \shortstack{\textbf{Number of Possible} \\ \textbf{Assignments}} & \multicolumn{4}{c}{\textbf{Number of Units (N) in Sample}} \\ 
            \cmidrule(lr){3-6}
            & & \textbf{4} & \textbf{8} & \textbf{16} & \textbf{32} \\ 
            \midrule
            \shortstack{Bernoulli \\ trial} & \( 2^N \) & 16 & 256 & 65,536 & \( 4.2 \times 10^9 \) \\[5pt]
            \shortstack{Completely \\ randomized \\ experiment} & \( \binom{N}{N/2} \) & 6 & 70 & 12,870 & \( 0.6 \times 10^9 \) \\[5pt]
            \shortstack{Stratified \\ randomized \\ experiment} & \( \binom{N/2}{N/4}^2 \) & 4 & 36 & 4,900 & \( 0.2 \times 10^9 \) \\[5pt]
            \shortstack{Paired \\ randomized \\ experiment} & \( 2^{N/2} \) & 4 & 16 & 256 & 65,536 \\ 
            \bottomrule
        \end{tabular}
    \end{table}
\end{frame}


\section{Inference  1: Fisher Exact P-Value}
% Slide 1: SUTVA Definition
\begin{frame}{Stable Unit Treatment Value Assumption (SUTVA)}
    \textbf{Assumption 1.1 (SUTVA):}
    
    \begin{itemize}
        
        \item For each unit, there are no different forms or versions of each treatment level that lead to different potential outcomes.\\
        
        \pause
        \vspace{0.5cm}
        \textbf{Example:}  Suppose a drug can be administered as a tablet or an injection. then SUTVA implies:

    \[
    Y_i(1_{\text{tablet}}) = Y_i(1_{\text{injection}})
    \]
        \pause
        \item The potential outcomes for any unit do not vary with the treatments assigned to other units.
        \pause
        \[
            Y(1)_i = Y(1,1)_i = Y(1,0)_i \quad \text{for } i = 1, 2
            \]
        
    \end{itemize}

    \vspace{0.5cm}
    \textbf{Example:} Under SUTVA, the potential outcomes for two units (1 and 2) would be consistent regardless of others' treatment status:
    
  
    
    \vspace{1cm} % Space for notes or additional explanations
\end{frame}

\begin{frame}{Fisher’s Exact P-Value}
    \begin{itemize}
        \item \textbf{Context} Inference under \textbf{physical randomization} for assessing causal effects.
        \pause
        \item \textbf{Sharp Null Hypothesis:} Fisher focused on testing the \emph{sharp null hypothesis} $$H0: Y_i(1) = Y_i(0) \quad \text{for all units } i = 1, \ldots, N. $$
        \pause
        \item \textbf{Exact p-Values:} The probability, under the $H0$, of observing a test statistic as extreme or more extreme than the one actually observed.
        \pause
        \item \textbf{Nonparametric Approach:} This method makes no assumptions about the distribution of the test statistic under the null hypothesis.(as t-test or ANOVA)
        \pause
        \item \textbf{Flexibility} Both $H0$ and test statistic can be defined in various ways, making the method widely applicable.
    \end{itemize}
\end{frame}

% Corrected Frame with Table
\begin{frame}{Table 5.3: Cough Frequency for the First Six Units from the Honey Study}
    \begin{table}[]
        \centering
        \begin{tabular}{@{}cccccc@{}} % Changed to 'cccccc' for six columns
            \toprule
            \textbf{Unit} & \multicolumn{2}{c}{\textbf{Potential Outcomes}} & \multicolumn{3}{c}{\textbf{Observed Variables}} \\ 
            % Total columns: 1 + 2 + 3 = 6
            & \textbf{\( Y_i(0) \)} & \textbf{\( Y_i(1) \)} & \textbf{\( D_i \)} & \textbf{\( X_i \)} & \textbf{\( Y_{\text{obs}} \)} \\ \midrule
            % The empty '&' at the start accounts for the first column under 'Unit'
            1 &  & 3 & 1 & 4 & 3 \\
            2 &  & 5 & 1 & 6 & 5 \\
            3 &  & 0 & 1 & 4 & 0 \\
            4 & 4 &  & 0 & 4 & 4 \\
            5 & 0 &  & 0 & 1 & 0 \\
            6 & 1 &  & 0 & 5 & 1 \\
            \bottomrule
        \end{tabular}
    \end{table}
\end{frame}

% Frame with Steps for the Exact Test
\begin{frame}{Steps for the Exact Test}
    \begin{enumerate}
        \item Define \( H_0 \), e.g., \( Y(1) = Y(0) \).
        \pause
        \item Define the test statistic, calculating the difference in average outcomes by treatment status:
            \[
            T_{\text{diff}} = \Bigg|\left( \frac{\sum_{i: D_i=1} Y_{\text{obs}, i}}{N_t} \right) - \left( \frac{\sum_{i: D_i=0} Y_{\text{obs}, i}}{N_c} \right)\Bigg|
            \]
        \pause
        \item Compute the observed \( T_{\text{diff, obs}} \) from the data.
        \pause
        \item Compute \( T_{\text{diff}, k} \) for all possible assignment vectors \( D_k\).
        \pause
        \item Compute the p-value: approximate the p-value by the fraction of these \( K \) statistics that are as extreme as, or more extreme than, the observed \( T_{\text{diff, obs}} \):
            \[
            p = \frac{1}{K} \sum_{k=1}^K 1\left\{ T_{\text{diff}, k} \geq T_{\text{diff, obs}} \right\}
            \]
        \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Homework: Fisher Exact P-Value}
    \begin{enumerate}
        \item Take the class and compute the Fisher Exact P-Value as before.
        \item Modify the test statistic and compute the rank statistic instead. Compare results
        \item Modify the null hypothesis and test the null hypothesis that the unit treatment effect is 10\% Compare results
        \item Compute Fisher Exact P-value using covariates (pp78 CIS)
    \end{enumerate}
\end{frame}
\section{Inference  2: Neyman's ATE Test}
\begin{frame}{Definition of Average Treatment Effect (ATE)}
    The \textbf{Average Treatment Effect (ATE)} is defined as:
    \[
    \tau_{fs} = \frac{1}{N} \sum_{i=1}^{N} \left[ Y_i(1) - Y_i(0) \right] = \overline{Y}(1) - \overline{Y}(0),
    \]
    where:
    \[
    \overline{Y}(1) = \frac{1}{N} \sum_{i=1}^{N} Y_i(1), \quad \overline{Y}(0) = \frac{1}{N} \sum_{i=1}^{N} Y_i(0).
    \]
    
    \vspace{2cm} % Space for your notes
    
    \end{frame}
    
    \begin{frame}{Definition of $\hat{\tau}_{\text{diff}}$}
    Define the estimator $\hat{\tau}_{\text{diff}}$ as the difference of the sample means:
    \[
    \hat{\tau}_{\text{diff}} = \overline{Y}_{\text{obs}, t} - \overline{Y}_{\text{obs}, c},
    \]
    where:
    \[
    \overline{Y}_{\text{obs}, t} = \frac{1}{N_t} \sum_{i: W_i=1} Y^{\text{obs}}_i, \quad \overline{Y}_{\text{obs}, c} = \frac{1}{N_c} \sum_{i: W_i=0} Y^{\text{obs}}_i.
    \]
    
    \vspace{2cm} % Space for your notes
    
    \end{frame}
    
    \begin{frame}{Theorem: Unbiasedness of $\hat{\tau}_{\text{diff}}$}
    \textbf{Theorem}: The estimator $\hat{\tau}_{\text{diff}}$ is an unbiased estimator of the average treatment effect $\tau_{fs}$.
    
    \vspace{0.5cm} % Small space before proof
    
    \textbf{Proof}:
    
    \vspace{4cm} % Space for your proof
    
    \end{frame}
    
    \begin{frame}{Definition of Sampling Variance of the Neyman Estimator}
    The sampling variance of $\hat{\tau}_{\text{diff}}$ is:
    \[
    V_W\left( \hat{\tau}_{\text{diff}} \right) = \frac{S_c^2}{N_c} + \frac{S_t^2}{N_t} - \frac{S_{tc}^2}{N},
    \]
    where:
    \[
    S_c^2 = \frac{1}{N - 1} \sum_{i=1}^{N} \left( Y_i(0) - \overline{Y}(0) \right)^2,
    \]
    \[
    S_t^2 = \frac{1}{N - 1} \sum_{i=1}^{N} \left( Y_i(1) - \overline{Y}(1) \right)^2,
    \]
    \[
    S_{tc}^2 = \frac{1}{N - 1} \sum_{i=1}^{N} \left( [Y_i(1) - Y_i(0)] - \tau_{fs} \right)^2.
    \]
    
    \vspace{1.5cm} % Space for your notes
    
    \end{frame}
    
    \begin{frame}{Theorem 6.2}
    \textbf{Theorem 6.2}: The sampling variance of $\hat{\tau}_{\text{diff}}$ is given by:
    \[
    V_W\left( \hat{\tau}_{\text{diff}} \right) = \frac{S_c^2}{N_c} + \frac{S_t^2}{N_t} - \frac{S_{tc}^2}{N}.
    \]
    
    \vspace{0.5cm} % Small space before proof
    
    \textbf{Intuition}:
    
    \vspace{4cm} % Space for your proof
    
    \end{frame}
    
    \begin{frame}{Theorem 6.3}
    \textbf{Theorem 6.3}: If the treatment effect $Y_i(1) - Y_i(0)$ is constant across units, then an unbiased estimator for the sampling variance is:
    \[
    \hat{V}_{\text{Neyman}} = \frac{\hat{s}_c^2}{N_c} + \frac{\hat{s}_t^2}{N_t},
    \]
    where $s_c^2$ and $s_t^2$ are sample variances calculated from the observed data.
    
    \vspace{0.5cm} % Small space before proof
    
    \textbf{Estimation}:
    
    \vspace{4cm} % Space for your proof
    
    \end{frame}
    
    \begin{frame}{Confidence Intervals}
    To construct a $(1 - \alpha) \times 100\%$ confidence interval for $\tau_{fs}$, we use:
    \[
    \text{CI}_{1 - \alpha}(\tau_{fs}) = \left[ \hat{\tau}_{\text{diff}} + z_{\alpha/2} \sqrt{\hat{V}}, \quad \hat{\tau}_{\text{diff}} + z_{1 - \alpha/2} \sqrt{\hat{V}} \right],
    \]
    where $z_{\alpha/2}$ is the $\alpha/2$ quantile of the standard normal distribution.
    
    \vspace{2cm} % Space for your notes
    
    \end{frame}
    
    \begin{frame}{Testing}
    To test the null hypothesis $H_0: \tau_{fs} = 0$ against the alternative $H_a: \tau_{fs} \ne 0$, we compute the test statistic:
    \[
    t = \frac{\hat{\tau}_{\text{diff}}}{\sqrt{\hat{V}}}.
    \]
    We compare $t$ to the critical values from the standard normal distribution.
    
    \vspace{2cm} % Space for your notes
    
    \end{frame}






\section{Covariates and Heterogeneity}

\begin{frame}{Classical Additive Approach: Improving Precision Under Linearity}
    We start with the assumption that the conditional expectation function is exactly linear:
    \[
    \mathbb{E}[Y \mid D, W] = D\alpha + \beta^\top X,
    \]
    where:
    \begin{itemize}
        \item $D$ is the treatment indicator.
        \item $W$ represents pre-treatment covariates.
        \item $X = (1, W^\top)^\top$ includes an intercept and $W$.
    \end{itemize}
    
    \vspace{1cm} % Space for your notes
    
    \end{frame}
    
    \begin{frame}{Centered Covariates and Covariate Balance}
    We assume that the covariates are centered:
    \[
    \mathbb{E}[W] = 0.
    \]
    By the assumption of covariate balance in randomized experiments:
    \[
    \mathbb{E}[W \mid D = 1] = \mathbb{E}[W \mid D = 0].
    \]
    
    \vspace{1cm} % Space for your notes
    
    \end{frame}
    
    \begin{frame}{Average Treatment Effect under Linearity}
    Using centered covariates, we have:
    \[
    \begin{aligned}
    \mathbb{E}[Y(0)] &= \mathbb{E}\left[ \mathbb{E}[Y \mid D=0, X] \right] = \beta_1, \\
    \mathbb{E}[Y(1)] &= \mathbb{E}\left[ \mathbb{E}[Y \mid D=1, X] \right] = \beta_1 + \alpha.
    \end{aligned}
    \]
    Therefore, the Average Treatment Effect (ATE) is:
    \[
    \delta = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)] = \alpha.
    \]
    
    \vspace{1cm} % Space for your notes
    
    \end{frame}
    
    \begin{frame}{Statistical Inference on the ATE}
    Even without assuming linearity, the projection coefficient $\alpha$ recovers the ATE $\delta$.
    
    Under regularity conditions, the Ordinary Least Squares (OLS) estimators satisfy:
    \[
    \sqrt{n}
    \begin{pmatrix}
    \hat{\alpha} - \alpha \\
    \hat{\beta}_1 - \beta_1
    \end{pmatrix}
    \overset{approx}{\sim}
    \mathcal{N}\left( 0, V \right),
    \]
    where the covariance matrix $V$ has components:
    \[
    \begin{aligned}
    V_{11} &= \frac{\mathbb{E}[\epsilon^2 \tilde{D}^2]}{\left( \mathbb{E}[\tilde{D}^2] \right)^2}, \\
    V_{22} &= \frac{\mathbb{E}[\epsilon^2 \tilde{1}^2]}{\left( \mathbb{E}[\tilde{1}^2] \right)^2}, \\
    V_{12} &= V_{21} = \frac{\mathbb{E}[\epsilon^2 \tilde{D} \tilde{1}]}{\mathbb{E}[\tilde{D}^2] \mathbb{E}[\tilde{1}^2]},
    \end{aligned}
    \]
    with:
    \begin{itemize}
        \item $\epsilon = Y - D\alpha - \beta^\top X$, where $\epsilon$ is uncorrelated with $D$ and $X$.
        \item $\tilde{D}$ is the residual of $D$ after partialling out $X$.
        \item $\tilde{1} = (1 - D)$ is the residual of 1 after partialling out $D$ and $W$.
    \end{itemize}
    
    \vspace{0.5cm} % Space for your notes
    
    \end{frame}
    
    \begin{frame}{Relative Average Treatment Effect}
    We can also perform inference on the Relative ATE $\alpha / \beta_1$.
    
    Using the Delta Method, we have:
    \[
    \sqrt{n} \left( \frac{\hat{\alpha}}{\hat{\beta}_1} - \frac{\alpha}{\beta_1} \right) \overset{approx}{\sim} \mathcal{N}\left( 0, G^\top V G \right),
    \]
    where:
    \[
    G = \begin{pmatrix}
    1 / \beta_1 \\
    - \alpha / \beta_1^2
    \end{pmatrix}.
    \]
    
    \vspace{1cm} % Space for your notes
    
    \end{frame}
    
    \begin{frame}{Improvement in Precision Under Linearity}
    When we do not include covariates, the OLS estimator $\bar{\alpha}$ estimates $\alpha$ in:
    \[
    Y = \alpha D + \beta_1 + U, \quad \mathbb{E}[U] = \mathbb{E}[U D] = 0,
    \]
    where:
    \[
    U = \beta^\top (X - \mathbb{E}[X]) + \epsilon.
    \]
    Under the linear model, including covariates improves precision:
    \[
    V_{11} \leq \bar{V}_{11},
    \]
    where $\bar{V}_{11}$ is the variance when covariates are omitted.
    
    \vspace{1cm} % Space for your notes
    
    \end{frame}
    
    \begin{frame}{Limitations Without Linearity}
    Without the linearity assumption, the improvement in precision is not guaranteed.
    
    \begin{itemize}
        \item The variance $V_{11}$ and $\bar{V}_{11}$ are not generally comparable.
        \item Including covariates may increase the standard errors if the linear model is misspecified.
    \end{itemize}
    
    \vspace{1cm} % Space for your notes
    
    \end{frame}
    
    \begin{frame}{The Interactive Approach: Capturing Heterogeneity}
    Consider the interactive linear model:
    \[
    \mathbb{E}[Y \mid D, W] = \alpha^\top X D + \beta^\top X.
    \]
    Here, the Conditional Average Treatment Effect (CATE) is:
    \[
    \delta(W) = \mathbb{E}[Y(1) \mid W] - \mathbb{E}[Y(0) \mid W] = \alpha^\top X.
    \]
    The Average Treatment Effect (ATE) is:
    \[
    \delta = \mathbb{E}[\delta(W)] = \alpha_1,
    \]
    where $\alpha_1$ is the first component of $\alpha$.
    
    \vspace{0.5cm} % Space for your notes
    
    \end{frame}
    
    \begin{frame}{Estimation and Inference with Interactions}
    The coefficient $\alpha$ is estimated from the linear projection:
    \[
    Y = \alpha^\top (D X) + \beta^\top X + \epsilon, \quad \epsilon \perp (X, D X).
    \]
    We can treat $D X$ as a vector of technical treatments and apply standard inference methods.
    
    \vspace{1cm} % Space for your notes
    
    \end{frame}
    
    \begin{frame}{Advantages of the Interactive Approach}
    \begin{itemize}
        \item Always delivers improvements in precision for estimating the ATE $\delta$, even if the linearity assumption does not hold.
        \item Allows for the discovery of treatment effect heterogeneity.
    \end{itemize}
    
    This result was demonstrated by Lin (2013).
    
    \vspace{1cm} % Space for your notes
    
    \end{frame}
    
    \begin{frame}{Conclusion}
    \begin{itemize}
        \item Including pre-treatment covariates can improve the precision of ATE estimates in RCTs.
        \item The classical additive approach relies on linearity and may not always improve precision.
        \item The interactive approach, which includes interactions between treatment and covariates, always improves precision and uncovers heterogeneity.
    \end{itemize}
    
    \vspace{1cm} % Space for your notes
    
    \end{frame}

    
\begin{frame} [allowframebreaks]
    \frametitle{References}
    \bibliographystyle{apalike}
    \bibliography{references}
\end{frame}

\end{document}


































 


